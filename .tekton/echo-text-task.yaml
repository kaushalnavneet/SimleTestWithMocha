apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: echo-text
spec:
  params:
    - name: deployBaseImage
      default: ibmcom/pipeline-base-image:latest
    - name: eventpayload
      type: string
    - name: run-db
      type: string
      default: 'True'
    - name: pipeline-debug
      type: string
      default: 'From Task'
  stepTemplate:
    env:
      - name: PIPELINE_ID
        valueFrom:
          fieldRef:
            fieldPath: metadata.annotations['devops.cloud.ibm.com/pipeline-id']
      - name: PIPELINE_RUN_URL
        valueFrom:
          fieldRef:
            fieldPath: metadata.annotations['devops.cloud.ibm.com/pipeline-run-url']
      - name: LOCATION
        valueFrom:
          configMapKeyRef:
            name: trigger-data
            key: location
  volumes:
    - name: trigger-data
      configMap:
        name: trigger-data
  results:
    - name: pass-in-apikey-as-result
      description: APIKEY passed in
    - name: passdown
      description: something to pass down
  steps:
  - name: prepare
    image: ibmcom/pipeline-base-image
    env:
      - name: EVENT_PAYLOAD
        value: $(params.eventpayload)
      - name: PIPELINE_DEBUG
        value: $(params.pipeline-debug)
    script: |
        #!/usr/bin/env bash
        echo "LOCATION is ${LOCATION}"
        echo "MESSAGE is ${MESSAGE}"
        echo "PIPELINE_DEBUG is ${PIPELINE_DEBUG}"
        echo "payload.json"
        cat /trigger-data/payload.json
        echo "header"
        cat /trigger-data/header
        echo "body.json"
        cat /trigger-data/body.json
        echo ${PIPELINE_ID}
        echo ${PIPELINE_RUN_URL}
        echo ${EVENT_PAYLOAD}
        printf "\n"
        echo ${EVENT_PAYLOAD} | jq .ref
        printf "\n"
        echo ${TEXT} | jq .good
        printf "\n"
        echo -n "${TEXT}" > $(results.passdown.path)
        sleep 3000s
        echo -n "${API_KEY}" > $(results.pass-in-apikey-as-result.path)
    volumeMounts:
      - mountPath: /trigger-data
        name: trigger-data
  - name: tons-of-log
    image: ibmcom/pipeline-base-image
    script: |
        #!/bin/bash
        count=0
        hash='####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################'
        while [ "$count" -lt 1 ]
        do
        case $((RANDOM % 3)) in
          (0) status=OK
                  ;;
          (1) status=TEMP
                  ;;
          (2) status=PERM
                  ;;
        esac

        case $((RANDOM % 4)) in
        (0) data='Amazon S3 (Simple Storage Service) is an online file storage web service offered by Amazon Web Services. Amazon S3 provides storage through web services interfaces (REST, SOAP, and BitTorrent).[1] Amazon launched S3, its first publicly available web service, in the United States in March 2006[2] and in Europe in November 2007.[3]';;
        (1) data='Amazon S3 is reported to store more than 2 trillion objects as of April 2013.[7] This is up from 102 billion objects as of March 2010,[8] 64 billion objects in August 2009,[9] 52 billion in March 2009,[10] 29 billion in October 2008,[5] 14 billion in January 2008, and 10 billion in October 2007.[11]';;
        (2) data='S3 uses include web hosting, image hosting, and storage for backup systems. S3 guarantees 99.9% monthly uptime service-level agreement (SLA),[12] that is, not more than 43 minutes of downtime per month.[13]';;
        (3) data='Details of S3'\''s design are not made public by Amazon, though it clearly manages data with an object storage architecture. According to Amazon, S3'\''s design aims to provide scalability, high availability, and low latency at commodity costs.';;
        esac

        part=$(
          printf '%s|%s|%d|%s|%s' \
          "20130101" \
          $(printf '%02d:%02d:%02d' $((RANDOM % 3 + 9)) $((RANDOM % 60)) $((RANDOM % 60)) ) \
          $((RANDOM % 2000 + 3000)) \
          "$status" \
          "$data"
        )
        printf '%.500s\n' "$part"'|'"$hash"
        count=$((count + 1))
        done
  - name: echo-text-step-0
    image: ibmcom/pipeline-base-image
    script: |
        #!/usr/bin/env bash
        echo ${TEXT} | jq .good
        printf "\n"
        cat /trigger-data/payload.json
        printf "\n"
        cat /trigger-data/payload.json | jq .ref
        printf "\n"
        cat /trigger-data/TEXT | jq .good
        printf "\n"
        echo "passdown is '${passdown}'"
        echo results.name.path is $(results.passdown.path)
        echo API_KEY is $(results.pass-in-apikey-as-result.path)
    volumeMounts:
      - mountPath: /trigger-data
        name: trigger-data
  - name: echo-text-step-1
    image: ibmcom/pipeline-base-image
    command: ["/bin/bash", "-c"]
    args:
        - |
          echo ${TEXT} | jq .good
          printf "\n"
          cat /trigger-data/payload.json
          printf "\n"
          cat /trigger-data/payload.json | jq .ref
          printf "\n"
          cat /trigger-data/TEXT | jq .good
          printf "\n"
          echo "passdown is '${passdown}'"
          echo results.name.path is $(results.passdown.path)
          echo API_KEY is $(results.pass-in-apikey-as-result.path)
    volumeMounts:
      - mountPath: /trigger-data
        name: trigger-data
  - name: echo-text-step-2
    image: $(params.deployBaseImage)
    command: ["/bin/sh", "-c"]
    args:
        - |
          echo ${TEXT} | jq .good
          printf "\n"
          cat /trigger-data/payload.json
          printf "\n"
          cat /trigger-data/payload.json | jq .ref
          printf "\n"
          cat /trigger-data/TEXT | jq .good
          printf "\n"
          echo "passdown is '${passdown}'"
          echo results.name.path is $(results.passdown.path)
          echo API_KEY is $(results.pass-in-apikey-as-result.path)
    volumeMounts:
      - mountPath: /trigger-data
        name: trigger-data